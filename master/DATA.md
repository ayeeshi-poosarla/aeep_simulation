# Data Storage Pipeline with Integrated Pre-Processing and AWS Upload

This project implements a data storage pipeline that processes data from a pandas DataFrame and then uploads the processed file to AWS Cloud Storage. The design streamlines the workflow by integrating the pre-processing step into the AWS upload script. When you run `aws_upload.py`, it will call the necessary function from `pre_processing.py` and perform the upload.

---

## 1. Prerequisites

### Software and Tools
- **Python 3.x**  
  Verify your installation:
  ```bash
  python3 --version
  ```
- **pip** – For installing dependencies.
- **(Optional) Virtual Environment**  
  It is recommended to use a virtual environment to manage project dependencies.

### AWS Configuration
- Ensure you have an active AWS account with the necessary permissions for S3 uploads.
- AWS credentials must be configured using one of the following methods:
  - AWS CLI configuration (see [AWS CLI Configuration](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html))
  - Environment variables: `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`

### Python Dependencies

The project uses the following packages:
- **pandas** – For DataFrame manipulation.
- **sqlalchemy** – For database interactions (if needed in data pre-processing).
- **awswrangler** – For interacting with AWS services (such as uploading to S3).

Create a `requirements.txt` file with the following contents:
```
pandas
sqlalchemy
awswrangler
```
Install the dependencies by running:
```bash
pip install -r requirements.txt
```

---

## 2. Project Structure

The recommended folder structure for this project is as follows:

```
/data_storage_project
   ├── pre_processing.py   # Contains functions to process data (e.g., transforming a pandas DataFrame).
   ├── aws_upload.py       # Script that calls a function from pre_processing.py and uploads the result to AWS Cloud Storage.
   ├── requirements.txt    # List of required Python packages.
   └── sample_data.csv     # (Optional) Example input data file.
```

> **Note:** In this design, there is no need to run `pre_processing.py` separately. The AWS upload script imports and uses the necessary pre-processing function.

---

## 3. Setup Instructions

### 3.1 Create and Activate a Virtual Environment (Optional)
1. **Create the Virtual Environment:**
   ```bash
   python3 -m venv venv
   ```
2. **Activate the Virtual Environment:**
   - On macOS/Linux:
     ```bash
     source venv/bin/activate
     ```
   - On Windows:
     ```bash
     venv\Scripts\activate
     ```

### 3.2 Install Dependencies
From the project directory, install the dependencies:
```bash
pip install -r requirements.txt
```

---

## 4. Running the Pipeline

Since the AWS upload script integrates the pre-processing function, you only need to execute one command:

### Run the AWS Upload Script
This will process your data and upload the output to your designated AWS S3 bucket.
```bash
python3 aws_upload.py
```
> **Note:** Before running, ensure your AWS credentials are set up and that any bucket names or configuration values in `aws_upload.py` (and potentially in `pre_processing.py`) are properly updated to reflect your AWS environment.

---

## 5. How It Works

1. **Pre-Processing (Handled by pre_processing.py)**
   - The `pre_processing.py` module contains a function that takes a pandas DataFrame as input, performs necessary transformations (e.g., cleaning, aggregating, or reformatting the data), and outputs a file (e.g., `processed_data.csv`).
   
2. **AWS Upload (Handled by aws_upload.py)**
   - The `aws_upload.py` script imports the pre-processing function from `pre_processing.py`.
   - It runs the pre-processing function, which prepares the data.
   - It then uploads the resulting file to AWS Cloud Storage using the `awswrangler` library.

---

## 6. Configuration Details

### In `aws_upload.py`
- **AWS Bucket and Region:**  
  Update the script with the target S3 bucket name and region.
- **File Paths:**  
  Ensure that the script references the output file generated by the pre-processing step (for example, `processed_data.csv`).

### In `pre_processing.py`
- **Data Input:**  
  Adjust the data source if needed (for instance, reading from `sample_data.csv` or initializing a DataFrame in the code).
- **Transformation Logic:**  
  Customize the data pre-processing logic as necessary to suit your project requirements.

---

## 7. Troubleshooting & Tips

- **Module Import Errors:**  
  Ensure that you're running the script from the correct directory and that the virtual environment (if used) is activated.
  
- **AWS Upload Issues:**  
  - Verify that your AWS credentials and IAM permissions are correctly set.
  - Check that the S3 bucket exists and that the specified region is correct.

- **Data Processing Errors:**  
  - Check that the input data conforms to the expected format.
  - Use logging or print statements to identify and resolve issues within the pre-processing function.

---

## 8. Future Enhancements

- **Enhanced Logging:**  
  Integrate a robust logging mechanism to capture detailed workflow information and errors.
- **Error Handling:**  
  Improve error handling in both the pre-processing and AWS upload steps.
- **CI/CD Integration:**  
  Automate the pipeline using continuous integration and deployment practices.